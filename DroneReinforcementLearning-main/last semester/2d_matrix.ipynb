{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PLEASE GO THE FOLLOWING LINK TO RUN THE CODE https://colab.research.google.com/drive/1u7PHpb2CAS7DMYpy9sBmvOhdYCzVwZuC?usp=sharing"
      ],
      "metadata": {
        "id": "65FXuZHyzKyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE ADAPTED FROM \"https://github.com/MJeremy2017/reinforcement-learning-implementation/blob/master/GridWorld/gridWorld.py\""
      ],
      "metadata": {
        "id": "lAu3X9cqQoBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"2D_Matrix.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1u7PHpb2CAS7DMYpy9sBmvOhdYCzVwZuC\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VPsR6QGxzKKc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "83015f51-03a2-4ff2-e809-85d9bc506c2c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2D_Matrix.ipynb\\n\\nAutomatically generated by Colaboratory.\\n\\nOriginal file is located at\\n    https://colab.research.google.com/drive/1u7PHpb2CAS7DMYpy9sBmvOhdYCzVwZuC\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "XhrcelZSzUsJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "These values can be changed to make the reinforcement learning code intersting.\n",
        "\n",
        ":params START: This indiccates the start value of the agent/player in the 2d grid.\n",
        ":params BOARD_ROWS: This indicates the number of rows in the 2d grid.\n",
        ":params BOARD_COLS: This indicates the number of columns in the 2d grid.\n",
        ":params WIN_STATE: This indicates the goal of the agent/player to be reached.\n",
        ":params Zombie: This indicates the position of the weaker powered obstacle\n",
        ":params Wolf: This indicates the position of the stronger powered obstacle\n",
        ":params obstacle: All the obstacles are stored in the list for printing it later\n",
        "\"\"\"\n",
        "START = (7,0)\n",
        "BOARD_ROWS = 8\n",
        "BOARD_COLS = 8\n",
        "WIN_STATE  = (0,7)\n",
        "Zombie = (0,6)\n",
        "Wolf = (3,5)\n",
        "obstacle = [Zombie,Wolf]"
      ],
      "metadata": {
        "id": "OZm5i-mo1c9z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State():\n",
        "  \"\"\"\n",
        "    This is a State class which controls all the cells state in the 2d grid.\n",
        "    In our 2d board depending upon the action by the agent the current state\n",
        "    can be changed or it can be a episode (win or lose).\n",
        "\n",
        "    :param START: Players start value anywhere in the board\n",
        "    :type START: tuple, Mandatory\n",
        "  \"\"\"\n",
        "  def __init__(self, state = START):\n",
        "    \"\"\"\n",
        "    Constructor Method\n",
        "    \"\"\"\n",
        "    self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
        "    self.state = state\n",
        "    self.isEnd = False\n",
        "\n",
        "  def giveReward(self):\n",
        "    \"\"\"\n",
        "    There are 3 obstacles in the 2d grid, and if any of the state falls under\n",
        "    the below mentioned category they get a reward\n",
        "    :return: Returns a reward value if it is equal to the any of the mentioned category\n",
        "    :rtype: int, this returns only one of the following value 1, -1 and -5   \n",
        "    \"\"\"\n",
        "    if self.state == WIN_STATE:\n",
        "        return 1\n",
        "    elif self.state == Zombie:\n",
        "        return -1\n",
        "    elif self.state == Wolf:\n",
        "        return -5\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "  def isEndFunc(self):\n",
        "    \"\"\"\n",
        "    This function is used to stop the reinforcement learning for the current round\n",
        "    if it falls under any of the following states WIN_STATE, zombie and wolf\n",
        "    :return: Returns a boolean value \"True\"\n",
        "    :rtype: bool\n",
        "    \"\"\"\n",
        "    if (self.state == WIN_STATE) or (self.state == Zombie) or (self.state == Wolf):\n",
        "      self.isEnd = True\n",
        "\n",
        "  def nxtPosition(self, action):\n",
        "    \"\"\"\n",
        "    Returns the next position state/cell value based on the agents action and if the next \n",
        "    state falls under in any of the teleport dicitonary keys the next state will be not up,\n",
        "    down, left and right rather it jumps multiple cells depending upon the dictionary value.\n",
        "    Similarly, there is a variabel called pull_back state, which pulls you back to the previous\n",
        "    state if your agent action resulted in any of the state present in the pull back list.\n",
        "\n",
        "    :param action: Contains any one of the following string value (up, down,left, right)\n",
        "    :param type: String\n",
        "    :return: Returns the agents tuple value if it not falls under any of the teleport dicitonary or\n",
        "    pull back method or else returns the agents desired next state.\n",
        "    :rtype: tuple \n",
        "    \"\"\"\n",
        "    teleport_dictionary={(4,2):(1,7),(1,2):(4,2),(3,4):(1,7)}\n",
        "    pull_back = [(5,2),(4,7)]\n",
        "    if action == 'U':\n",
        "        nxtState = (self.state[0] - 1, self.state[1])\n",
        "    elif action == 'D':\n",
        "        nxtState = (self.state[0] + 1, self.state[1])\n",
        "    elif action == 'L':\n",
        "        nxtState = (self.state[0], self.state[1] - 1)\n",
        "    else:\n",
        "        nxtState = (self.state[0], self.state[1] + 1)\n",
        "\n",
        "    # if next state legal\n",
        "    if (nxtState[0] >= 0) and (nxtState[0] <= (BOARD_ROWS -1)):\n",
        "        if (nxtState[1] >= 0) and (nxtState[1] <= (BOARD_COLS -1)):\n",
        "          #Checks if the next state falls in teleport keys\n",
        "          if nxtState in teleport_dictionary.keys():\n",
        "            return teleport_dictionary[(nxtState[0],nxtState[1])]\n",
        "          # Checks if the next state is present in the pull back list\n",
        "          if nxtState in pull_back:\n",
        "            \n",
        "            return (self.state[0], self.state[1])\n",
        "          return nxtState\n",
        "    return self.state"
      ],
      "metadata": {
        "id": "6eEflo9LmdGn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  \"\"\"\n",
        "  This class is responsible for choosing the action until it reaches the episode (Win or Lose). \n",
        "  This class depends upon the state class to move to the next state. Depending upon each episode\n",
        "  the agent learns and during the next play takes the best action.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Constructor Method\n",
        "    \"\"\" \n",
        "    self.states = []\n",
        "    self.actions = ['U', 'D', 'L', 'R']\n",
        "    self.dictionary = {\n",
        "    (0,0):('D', 'R'),\n",
        "    (0,1):('D', 'R', 'L'),\n",
        "    (0,2):('D', 'L', 'R'),\n",
        "    (0,3):('D', 'L'),\n",
        "    (0,4): ('D','R'),\n",
        "    (1,0):('D', 'U', 'R'),\n",
        "    (1,1):('D', 'R', 'L', 'U'),\n",
        "    (1,3):('D', 'L', 'U'),\n",
        "    (2,0):('U', 'R'),\n",
        "    (2,1):('U', 'L', 'R'),\n",
        "    (4,4):('U','U')\n",
        "     }\n",
        "    self.State = State()\n",
        "    self.lr = 0.3\n",
        "    self.exp_rate = 0.4\n",
        "\n",
        "    # initial state reward\n",
        "    self.state_values = {}\n",
        "    for i in range(BOARD_ROWS):\n",
        "        for j in range(BOARD_COLS):\n",
        "            self.state_values[(i, j)] = 0\n",
        "\n",
        "  def chooseAction(self):\n",
        "    \"\"\"\n",
        "    The agent chooses the action to exploit all the cells and then choose the best possible\n",
        "    action or the agent greedy method can be controlled by a exploitation value called \n",
        "    exp_rate which is 0.2 everytime. \n",
        "\n",
        "    :return: Returns any one of the following value \"up\", \"down\", \"left\" or \"right\" \n",
        "    :rtype: String \n",
        "    \"\"\"\n",
        "    # choose action with most expected value\n",
        "    mx_nxt_reward = 0\n",
        "    action = \"\" \n",
        "\n",
        "    if np.random.uniform(0, 1) <= self.exp_rate:\n",
        "      if self.State.state in self.dictionary.keys():\n",
        "        return self.dictionary[self.State.state]\n",
        "      else:\n",
        "        action = np.random.choice(self.actions)\n",
        "    else:\n",
        "        # greedy action\n",
        "      for a in self.actions:\n",
        "          # if the action is deterministic\n",
        "        nxt_reward = self.state_values[self.State.nxtPosition(a)]\n",
        "        if nxt_reward >= mx_nxt_reward:\n",
        "          action = a\n",
        "          mx_nxt_reward = nxt_reward\n",
        "    return action\n",
        "\n",
        "  def takeAction(self, action):\n",
        "    \"\"\"\n",
        "    The agent takes the neccessary action (move up,down,left or right) after the action has \n",
        "    been chosen from the Class Agent:chooseAction() method.\n",
        "\n",
        "    :param action: Contains any one of the following string value (up, down,left, right)\n",
        "    :param type: String\n",
        "\n",
        "    :return: Updates the state class with new position as the START value\n",
        "    :rtype: Returns the state object from (Class: STATE)\n",
        "    \"\"\"\n",
        "    position = self.State.nxtPosition(action)\n",
        "    return State(state=position)\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    This method resets all the state value and erases the state memory for the entire\n",
        "    game played\n",
        "    \"\"\"\n",
        "    self.states = []\n",
        "    self.State = State()\n",
        "\n",
        "  def play(self, rounds=10):\n",
        "    \"\"\"\n",
        "    This method is responsible for stopping the game if it reaches the episode and then by the \n",
        "    help of dictionary the agent backtracks and updates the reward value to all the previous states/cells \n",
        "    where it came from using a deterministic reinforcement learning formula called value iteration. By updating\n",
        "    the states using reward for each episdode will help the agent to reach its goal or the end state\n",
        "    withlin less steps when the game is played next time.\n",
        "\n",
        "    :param rounds: Total number of games to be played by the agent\n",
        "    :param type: int\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    while i < rounds:\n",
        "\n",
        "      if self.State.isEnd:\n",
        "        # back propagate\n",
        "        reward = self.State.giveReward()\n",
        "        # explicitly assign end state to reward values\n",
        "        self.state_values[self.State.state] = reward  # this is optional\n",
        "        #print(\"Game End Reward\", reward)\n",
        "        for s in reversed(self.states):\n",
        "          reward = self.state_values[s] + self.lr * (reward - self.state_values[s])\n",
        "          self.state_values[s] = round(reward, 3)\n",
        "        self.reset()\n",
        "        i += 1\n",
        "      else:\n",
        "        action = self.chooseAction()\n",
        "        # append trace\n",
        "        for s in action:\n",
        "          self.states.append(self.State.nxtPosition(action))\n",
        "          #print(\"current position {} action {}\".format(self.State.state, action))\n",
        "          # by taking the action, it reaches the next state\n",
        "          self.State = self.takeAction(action)\n",
        "          # mark is end\n",
        "          self.State.isEndFunc()\n",
        "          #print(\"nxt state\", self.State.state)\n",
        "          #print(\"---------------------\")\n",
        "  def showValues(self):\n",
        "    for i in range(0, BOARD_ROWS):\n",
        "      print('------------------------------------------------------------------------')\n",
        "      out = '| '\n",
        "      for j in range(0, BOARD_COLS):\n",
        "        if (i,j) in obstacle:\n",
        "          out += 'OBSTACLE'.ljust(1) + ' | '\n",
        "        elif (i,j) == WIN_STATE:\n",
        "          out += 'GOAL'.ljust(2) + ' | '\n",
        "        elif (i,j) == START:\n",
        "          out += 'START'.ljust(6) + ' | '\n",
        "        else:\n",
        "          out += str((i, j)).ljust(6) + ' | '\n",
        "      print(out)\n",
        "    print('------------------------------------------------------------------------\\n\\n')\n",
        "    \n",
        "    for i in range(0, BOARD_ROWS):\n",
        "      print('------------------------------------------------------------------------')\n",
        "      out = '| '\n",
        "      for j in range(0, BOARD_COLS):\n",
        "          out += str(self.state_values[(i, j)]).ljust(6) + ' | '\n",
        "      print(out)\n",
        "    print('------------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "gnt-Cx08nh23"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  \"\"\"\n",
        "  As you can see in the below matrix, the agent start from the position (row:7,column:0) and it reaches the end goal (row:1,column:7) where it gets a \n",
        "  reward of 1.0 (mentioned in the second matrix). The agent takes the maximum state values at each step or takes the random action to reduce the exploitation time when the agent starts\n",
        "  the game again to play.\n",
        "  \"\"\"\n",
        "  ag = Agent()\n",
        "  ag.play(1000)\n",
        "  print(ag.showValues())"
      ],
      "metadata": {
        "id": "QQKf8OMq_XTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492be23e-99bd-4533-a782-65c69ec4ebaa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "| (0, 0) | (0, 1) | (0, 2) | (0, 3) | (0, 4) | (0, 5) | OBSTACLE | GOAL | \n",
            "------------------------------------------------------------------------\n",
            "| (1, 0) | (1, 1) | (1, 2) | (1, 3) | (1, 4) | (1, 5) | (1, 6) | (1, 7) | \n",
            "------------------------------------------------------------------------\n",
            "| (2, 0) | (2, 1) | (2, 2) | (2, 3) | (2, 4) | (2, 5) | (2, 6) | (2, 7) | \n",
            "------------------------------------------------------------------------\n",
            "| (3, 0) | (3, 1) | (3, 2) | (3, 3) | (3, 4) | OBSTACLE | (3, 6) | (3, 7) | \n",
            "------------------------------------------------------------------------\n",
            "| (4, 0) | (4, 1) | (4, 2) | (4, 3) | (4, 4) | (4, 5) | (4, 6) | (4, 7) | \n",
            "------------------------------------------------------------------------\n",
            "| (5, 0) | (5, 1) | (5, 2) | (5, 3) | (5, 4) | (5, 5) | (5, 6) | (5, 7) | \n",
            "------------------------------------------------------------------------\n",
            "| (6, 0) | (6, 1) | (6, 2) | (6, 3) | (6, 4) | (6, 5) | (6, 6) | (6, 7) | \n",
            "------------------------------------------------------------------------\n",
            "| START  | (7, 1) | (7, 2) | (7, 3) | (7, 4) | (7, 5) | (7, 6) | (7, 7) | \n",
            "------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "------------------------------------------------------------------------\n",
            "| 0      | 0      | 0      | 0      | -0.304 | -0.548 | -1.0   | 1.0    | \n",
            "------------------------------------------------------------------------\n",
            "| 0      | 0      | 0      | 0.525  | 0.222  | -0.055 | 0.92   | 0.982  | \n",
            "------------------------------------------------------------------------\n",
            "| 0.831  | 0.846  | 0.81   | 0.747  | 0.802  | -0.491 | 0.401  | 0.916  | \n",
            "------------------------------------------------------------------------\n",
            "| 0.856  | 0.951  | 0.875  | 0.804  | 0      | -5.0   | 0.578  | 0.7    | \n",
            "------------------------------------------------------------------------\n",
            "| 0.941  | 0.973  | 0.278  | 0.815  | 0.799  | 0.711  | 0.645  | 0      | \n",
            "------------------------------------------------------------------------\n",
            "| 0.959  | 0.967  | 0      | 0.848  | 0.835  | 0.795  | 0.671  | 0.255  | \n",
            "------------------------------------------------------------------------\n",
            "| 0.958  | 0.962  | 0.918  | 0.883  | 0.861  | 0.808  | 0.698  | 0.592  | \n",
            "------------------------------------------------------------------------\n",
            "| 0.946  | 0.942  | 0.909  | 0.848  | 0.843  | 0.783  | 0.636  | 0.627  | \n",
            "------------------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actions = {\n",
        "    (0,0):('D', 'R'),\n",
        "    (0,1):('D', 'R', 'L'),\n",
        "    (0,2):('D', 'L', 'R'),\n",
        "    (0,3):('D', 'L'),\n",
        "    (1,0):('D', 'U', 'R'),\n",
        "    (1,1):('D', 'R', 'L', 'U'),\n",
        "    (1,3):('D', 'L', 'U'),\n",
        "    (2,0):('U', 'R'),\n",
        "    (2,1):('U', 'L', 'R')\n",
        "     }\n",
        "for s in actions.keys():\n",
        "  print(np.random.choice(actions[s]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTRvYvpcip8D",
        "outputId": "5af90f51-e6cc-48f2-8f39-3e978f03aa34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D\n",
            "D\n",
            "D\n",
            "D\n",
            "R\n",
            "L\n",
            "U\n",
            "U\n",
            "L\n"
          ]
        }
      ]
    }
  ]
}
